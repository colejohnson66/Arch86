%YAML 1.2
---
title: Add Packed Single-Precision Floating-Point Values
opcode:
  - opcode: NP 0F 58 /r
    mnemonic: ADDPS \i{xmm1}, \i{xmm2/m128}
    encoding: LEGACY
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: SSE
    description: >-
      Adds packed single-precision floating-point values from \i{xmm2/m128} and \i{xmm1}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.128.0F.WIG 58 /r
    mnemonic: VADDPS \i{xmm1}, \i{xmm2}, \i{xmm3/m128}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      Adds packed single-precision floating-point values from \i{xmm3/m128} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.256.0F.WIG 58 /r
    mnemonic: VADDPS \i{ymm1}, \i{ymm2}, \i{ymm3/m256}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      Adds packed single-precision floating-point values from \i{ymm3/m256} and \i{ymm2}.
      Stores the result in \i{ymm1}.
  - opcode: EVEX.128.0F.W0 58 /r
    mnemonic: VADDPS \i{xmm1} {k1}{z}, \i{xmm2}, \i{xmm3/m128/m32bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: [AVX512VL, AVX512F]
    description: >-
      Adds packed single-precision floating-point values from \i{xmm3/m128/m32bcst} and \i{xmm2}.
      Stores the result in \i{xmm1} using writemask \i{k1}.
  - opcode: EVEX.256.0F.W0 58 /r
    mnemonic: VADDPS \i{ymm1} {k1}{z}, \i{ymm2}, \i{ymm3/m256/m32bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: [AVX512VL, AVX512F]
    description: >-
      Adds packed single-precision floating-point values from \i{ymm3/m256/m32bcst} and \i{ymm2}.
      Stores the result in \i{ymm1} using writemask \i{k1}.
  - opcode: EVEX.512.0F.W0 58 /r
    mnemonic: VADDPS \i{zmm1} {k1}{z}, \i{zmm2}, \i{zmm3/m512/m32bcst} {er}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX512F
    description: >-
      Adds packed single-precision floating-point values from \i{zmm3/m512/m32bcst} and \i{zmm2}.
      Stores the result in \i{zmm1} using writemask \i{k1}.
encoding:
  operands: 3
  hasTuple: true
  encodings:
    LEGACY:
      - N/A
      - ModRM.reg[rw]
      - ModRM.r/m[r]
      - ""
    VEX:
      - N/A
      - ModRM.reg[w]
      - VEX.vvvv[r]
      - ModRM.r/m[r]
    EVEX:
      - Full
      - ModRM.reg[w]
      - EVEX.vvvv[r]
      - ModRM.r/m[r]
bitEncoding:
  list:
    - form: xmmreg3(2) into xmmreg3(1)
      bits:
        - \bits{0F}
        - \bits{58}
        - \bits{11 xmmreg3(1) xmmreg3(2)}
    - form: memory into xmmreg3
      bits:
        - \bits{0F}
        - \bits{58}
        - \bits{mod xmmreg3 r/m}
description: >-
  The \c{(V)ADDPS} instruction adds four, eight, or sixteen single-precision floating-point values from the first source operand to the second source operand.
  The result is stored in in the destination operand.

  All versions \i{except} the legacy SSE version zero the unused upper SIMD register bits.
operation: |-
  public void ADDPS(SimdF32 dest, SimdF32 src)
  {
    dest[0] += src[0];
    dest[1] += src[1];
    dest[2] += src[2];
    dest[3] += src[3];
    // dest[4..Simd.Max] (unmodified)
  }

  void VADDPS_Vex(SimdF32 dest, SimdF32 src1, SimdF32 src2, int kl)
  {
    for (int n = 0; i < kl, i++)
      dest[n] = src1[n] + src2[n];
    dest[kl..Simd.MAX] = 0;
  }
  public void VADDPS_Vex128(SimdF32 dest, SimdF32 src1, SimdF32 src2)
  {
    VADDPS_Vex(dest, src1, src2, 4);
  }
  public void VADDPS_Vex256(SimdF32 dest, SimdF32 src1, SimdF32 src2)
  {
    VADDPS_Vex(dest, src1, src2, 8);
  }

  void VADDPS_EvexMemory(SimdF32 dest, SimdF32 src1, SimdF32 src2, KMask k, int kl)
  {
    for (int n = 0; i < kl, i++)
    {
      if (k[n])
      {
        if (EVEX.b)
          dest[n] = src1[n] + src2[0];
        else
          dest[n] = src1[n] + src2[n];
      }
      else if (EVEX.z)
      {
        dest[n] = 0;
      }
    }
    dest[kl..Simd.MAX] = 0;
  }
  public void VADDPS_Evex128Memory(SimdF32 dest, SimdF32 src1, SimdF32 src2, KMask k)
  {
    VADDPS_EvexMemory(dest, src1, src2, k, 4);
  }
  public void VADDPS_Evex256Memory(SimdF32 dest, SimdF32 src1, SimdF32 src2, KMask k)
  {
    VADDPS_EvexMemory(dest, src1, src2, k, 8);
  }
  public void VADDPS_Evex512Memory(SimdF32 dest, SimdF32 src1, SimdF32 src2, KMask k)
  {
    VADDPS_EvexMemory(dest, src1, src2, k, 16);
  }

  void VADDPS_EvexRegister(SimdF32 dest, SimdF32 src1, SimdF32 src2, KMask k, int kl)
  {
    if (kl == 8 && EVEX.b)
      SetRoundingForThisInstruction(EVEX.rc);
    else
      SetRoundingForThisInstruction(MXCSR.rc);

    for (int n = 0; i < kl, i++)
    {
      int vi = n * 64;
      int viNext = vi + 63;

      if (k[n])
        dest[n] = src1[n] + src2[n];
      else if (EVEX.z)
        dest[n] = 0;
    }
    dest[kl..Simd.MAX] = 0;
  }
  public void VADDPS_Evex128Register(SimdF32 dest, SimdF32 src1, SimdF32 src2, KMask k)
  {
    VADDPS_EvexRegister(dest, src1, src2, k, 4);
  }
  public void VADDPS_Evex256Register(SimdF32 dest, SimdF32 src1, SimdF32 src2, KMask k)
  {
    VADDPS_EvexRegister(dest, src1, src2, k, 8);
  }
  public void VADDPS_Evex512Register(SimdF32 dest, SimdF32 src1, SimdF32 src2, KMask k)
  {
    VADDPS_EvexRegister(dest, src1, src2, k, 16);
  }
intrinsicsC: |-
  __m128 _mm_add_ps(__m128 a, __m128 b)
  __m128 _mm_mask_add_ps(__m128d s, __mmask8 k, __m128 a, __m128 b)
  __m128 _mm_maskz_add_ps(__mmask8 k, __m128 a, __m128 b)

  __m256 _mm256_add_ps(__m256 a, __m256 b)
  __m256 _mm256_mask_add_ps(__m256 s, __mmask8 k, __m256 a, __m256 b)
  __m256 _mm256_maskz_add_ps(__mmask8 k, __m256 a, __m256 b)

  __m512 _mm512_add_ps(__m512 a, __m512 b)
  __m512 _mm512_add_round_ps(__m512 a, __m512 b, int32_t rounding)
  __m512 _mm512_mask_add_ps(__m512 s, __mmask16 k, __m512 a, __m512 b)
  __m512 _mm512_mask_add_round_ps(__m512 s, __mmask16 k, __m512 a, __m512 b, int32_t rounding)
  __m512 _mm512_maskz_add_ps(__mmask16 k, __m512 a, __m512 b)
  __m512 _mm512_maskz_add_round_ps(__mmask16 k, __m512 a, __m512 b, int32_t rounding)
exceptions:
  floating: Overflow, Underflow, Invalid, Precision, Denormal
  other:
    - "VEX encoded form: see Exceptions Type 2."
    - "EVEX encoded form: see Exceptions Type E2."
