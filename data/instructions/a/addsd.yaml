%YAML 1.2
---
title: Add Scalar Double-Precision Floating-Point Value
opcode:
  - opcode: F2 0F 58 /r
    mnemonic: ADDSD \i{xmm1}, \i{xmm2/m64}
    encoding: LEGACY
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: SSE2
    description: >-
      Adds a single double-precision floating-point value from \i{xmm2/m64} and \i{xmm1}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.LIG.F2.0F.WIG 58 /r
    mnemonic: VADDSD \i{xmm1}, \i{xmm2}, \i{xmm3/m64}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      Adds a single double-precision floating-point value from \i{xmm3/m64} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: EVEX.LIG.F2.0F.W1 58 /r
    mnemonic: VADDSD \i{xmm1} {k1}{z}, \i{xmm2}, \i{xmm3/m64} {er}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX512F
    description: >-
      Adds a single double-precision floating-point value from \i{xmm3/m64} and \i{xmm2}.
      Stores the result in \i{xmm1} using writemask \i{k1}.
encoding:
  operands: 3
  hasTuple: true
  encodings:
    LEGACY:
      - N/A
      - ModRM.reg[rw]
      - ModRM.r/m[r]
      - ""
    VEX:
      - N/A
      - ModRM.reg[w]
      - VEX.vvvv[r]
      - ModRM.r/m[r]
    EVEX:
      - Tuple1 Scalar
      - ModRM.reg[w]
      - EVEX.vvvv[r]
      - ModRM.r/m[r]
description: >-
  The \c{(V)ADDSD} instruction adds a single double-precision floating-point from the first source operand and the second source operand.
  The result is stored in in the destination operand.

  The legacy SSE version (\c{ADDSD}) works on the lowest 64 bits and leaves all others unchanged.

  The AVX versions (\c{VADDSD}) work on the lowest 64 bits, but also copy the next 64 bits from the first source operand to the destination.
  Afterwards, all unchanged bits in the destination are zeroed.

  \canned{vexLShouldBe0}
operation: |-
  public void ADDSD(SimdF64 dest, SimdF64 src)
  {
    dest[0] += src[0];
    // dest[1..SimdF64.MAX] (unchanged)
  }

  public void VADDSD_Vex(SimdF64 dest, SimdF64 src1, SimdF64 src2)
  {
    dest[0] = src1[0] + src2[0];
    dest[1] = src1[1];
    dest[2..SimdF64.MAX] = 0;
  }

  public void VADDSD_EvexMemory(SimdF64 dest, SimdF64 src1, SimdF64 src2, KMask k)
  {
    if (k[0])
      dest[0] = src1[0] + src2[0];
    else if (EVEX.z)
      dest[0] = 0;
    dest[1] = src1[1];
    dest[2..SimdF64.MAX] = 0;
  }

  public void VADDSD_EvexRegister(SimdF64 dest, SimdF64 src1, SimdF64 src2, KMask k)
  {
    if (EVEX.b)
      SetRoundingForThisInstruction(EVEX.rc);
    else
      SetRoundingForThisInstruction(MXCSR.rc);

    if (k[0])
      dest[0] = src1[0] + src2[0];
    else if (EVEX.z)
      dest[0] = 0;
    dest[1] = src1[1];
    dest[2..SimdF64.MAX] = 0;
  }
intrinsics: |-
  __m128d _mm_add_sd(__m128d a, __m128d b)
  __m128d _mm_add_round_sd(__m128d a, __m128d b, int32_t rounding)
  __m128d _mm_mask_add_sd(__m128d s, __mmask8 k, __m128d a, __m128d b)
  __m128d _mm_mask_add_round_sd(__m128d s, __mmask8 k, __m128d a, __m128d b, int32_t rounding)
  __m128d _mm_maskz_add_sd(__mmask8 k, __m128d a, __m128d b)
  __m128d _mm_maskz_add_round_sd (__mmask8 k, __m128d a, __m128d b, int32_t rounding)
exceptions:
  floating: Overflow, Underflow, Invalid, Precision, Denormal
  other:
    - \exceptionVex{2}
    - \exceptionEvex{E2}
