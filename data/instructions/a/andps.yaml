%YAML 1.2
---
title: Logical AND Packed Single-Precision Floating-Point Values
opcode:
  - opcode: NP 0F 54 /r
    mnemonic: ANDPS \i{xmm1}, \i{xmm2/m128}
    encoding: LEGACY
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: SSE2
    description: >-
      ANDs packed single-precision floating-point values from \i{xmm2/m128} and \i{xmm1}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.128.0F.WIG 54 /r
    mnemonic: VANDPS \i{xmm1}, \i{xmm2}, \i{xmm3/m128}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      ANDs packed single-precision floating-point values from \i{xmm3/m128} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.256.0F.WIG 54 /r
    mnemonic: VANDPS \i{ymm1}, \i{ymm2}, \i{ymm3/m256}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      ANDs packed single-precision floating-point values from \i{ymm3/m256} and \i{ymm2}.
      Stores the result in \i{ymm1}.
  - opcode: EVEX.128.0F.W0 54 /r
    mnemonic: VANDPS \i{xmm1} {k1}{z}, \i{xmm2}, \i{xmm3/m128/m32bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: [AVX512VL, AVS512DQ]
    description: >-
      ANDs packed single-precision floating-point values from \i{xmm3/m128/m32bcst} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: EVEX.256.0F.W0 54 /r
    mnemonic: VANDPS \i{ymm1} {k1}{z}, \i{ymm2}, \i{ymm3/m256/m32bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: [AVX512VL, AVS512DQ]
    description: >-
      ANDs packed single-precision floating-point values from \i{ymm3/m256/m32bcst} and \i{ymm2}.
      Stores the result in \i{ymm1}.
  - opcode: EVEX.512.0F.W0 54 /r
    mnemonic: VANDPS \i{zmm1} {k1}{z}, \i{zmm2}, \i{zmm3/m512/m32bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX512DQ
    description: >-
      ANDs packed single-precision floating-point values from \i{zmm3/m512/m32bcst} and \i{zmm2}.
      Stores the result in \i{zmm1}.
encoding:
  operands: 3
  hasTuple: true
  encodings:
    LEGACY:
      - N/A
      - ModRM.reg[rw]
      - ModRM.r/m[r]
      - ""
    VEX:
      - N/A
      - ModRM.reg[rw]
      - VEX.vvvv[r]
      - ModRM.r/m[r]
    EVEX:
      - Full
      - ModRM.reg[rw]
      - EVEX.vvvv[r]
      - ModRM.r/m[r]
bitEncoding:
  list:
    - form: xmmreg3(2) into xmmreg3(1)
      bits:
        - \bits{0F}
        - \bits{54}
        - \bits{11 xmmreg3(1) xmmreg3(2)}
    - form: memory into xmmreg3
      bits:
        - \bits{0F}
        - \bits{54}
        - \bits{mod xmmreg3 r/m}
description: >-
  The \c{(V)ANDPS} instruction ANDs four, eight, or sixteen 32 bit integer values from the first source operand to the second source operand.
  The result is stored in in the destination operand.

  This instruction, despite being named as if it operates on floating-point numbers, internally operates on 32 bit integers.
  The "Operation" section below has been updated to reflect this (using \c{SimdU32} instead of \c{SimdF32}).

  All versions \i{except} the legacy SSE version zero the unused upper SIMD register bits.
operation: |-
  public void ANDPS(SimdU32 dest, SimdU32 src)
  {
    dest[0] &= src[0];
    dest[1] &= src[1];
    dest[2] &= src[2];
    dest[3] &= src[3];
    // dest[4..Simd.Max] (unmodified)
  }

  void VANDPS_Vex(SimdU32 dest, SimdU32 src1, SimdU32 src2, int kl)
  {
    for (int n = 0; n < kl, n++)
      dest[n] = src1[n] & src2[n];
    dest[kl..Simd.MAX] = 0;
  }
  public void VANDPS_Vex128(SimdU32 dest, SimdU32 src1, SimdU32 src2)
  {
    VANDPS_Vex(dest, src1, src2, 4);
  }
  public void VANDPS_Vex256(SimdU32 dest, SimdU32 src1, SimdU32 src2)
  {
    VANDPS_Vex(dest, src1, src2, 8);
  }

  void VANDPS_EvexMemory(SimdU32 dest, SimdU32 src1, SimdU32 src2, KMask k, int kl)
  {
    for (int n = 0; n < kl, n++)
    {
      if (k[n])
      {
        if (EVEX.b)
          dest[n] = src1[n] & src2[0];
        else
          dest[n] = src1[n] & src2[n];
      }
      else if (EVEX.z)
      {
        dest[n] = 0;
      }
    }
    dest[kl..Simd.MAX] = 0;
  }
  public void VANDPS_Evex128Memory(SimdU32 dest, SimdU32 src1, SimdU32 src2, KMask k)
  {
    VANDPS_EvexMemory(dest, src1, src2, k, 4);
  }
  public void VANDPS_Evex256Memory(SimdU32 dest, SimdU32 src1, SimdU32 src2, KMask k)
  {
    VANDPS_EvexMemory(dest, src1, src2, k, 8);
  }
  public void VANDPS_Evex512Memory(SimdU32 dest, SimdU32 src1, SimdU32 src2, KMask k)
  {
    VANDPS_EvexMemory(dest, src1, src2, k, 16);
  }

  void VANDPS_EvexRegister(SimdU32 dest, SimdU32 src1, SimdU32 src2, KMask k, int kl)
  {
    for (int n = 0; n < kl, n++)
    {
      if (k[n])
        dest[n] = src1[n] & src2[n];
      else if (EVEX.z)
        dest[n] = 0;
    }
    dest[kl..Simd.MAX] = 0;
  }
  public void VANDPS_Evex128Register(SimdU32 dest, SimdU32 src1, SimdU32 src2, KMask k)
  {
    VANDPS_EvexRegister(dest, src1, src2, k, 4);
  }
  public void VANDPS_Evex256Register(SimdU32 dest, SimdU32 src1, SimdU32 src2, KMask k)
  {
    VANDPS_EvexRegister(dest, src1, src2, k, 8);
  }
  public void VANDPS_Evex512Register(SimdU32 dest, SimdU32 src1, SimdU32 src2, KMask k)
  {
    VANDPS_EvexRegister(dest, src1, src2, k, 16);
  }
intrinsics: |-
  __m128d _mm_and_ps(__m128d a, __m128d b)
  __m128d _mm_mask_and_ps(__m128d s, __mmask16 k, __m128d a, __m128d b)
  __m128d _mm_maskz_and_ps(__mmask16 k, __m128d a, __m128d b)

  __m256d _mm256_and_ps(__m256d a, __m256d b)
  __m256d _mm256_mask_and_ps(__m256d s, __mmask16 k, __m256d a, __m256d b)
  __m256d _mm256_maskz_and_ps(__mmask16 k, __m256d a, __m256d b)

  __m512d _mm512_and_ps(__m512d a, __m512d b)
  __m512d _mm512_mask_and_ps(__m512d s, __mmask16 k, __m512d a, __m512d b)
  __m512d _mm512_maskz_and_ps(__mmask16 k, __m512d a, __m512d b)
exceptions:
  floating: None
  other:
    - "VEX encoded form: see Exceptions Type 4."
    - "EVEX encoded form: see Exceptions Type E4."
