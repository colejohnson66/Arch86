%YAML 1.2
---
title: Add Packed Double-Precision Floating-Point Values
opcode:
  - opcode: 66 0F 58 /r
    mnemonic: ADDPD \i{xmm1}, \i{xmm2/m128}
    encoding: LEGACY
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: SSE2
    description: >-
      Adds packed double-precision floating-point values from \i{xmm2/m128} and \i{xmm1}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.128.66.0F.WIG 58 /r
    mnemonic: VADDPD \i{xmm1}, \i{xmm2}, \i{xmm3/m128}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      Adds packed double-precision floating-point values from \i{xmm3/m128} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.256.66.0F.WIG 58 /r
    mnemonic: VADDPD \i{ymm1}, \i{ymm2}, \i{ymm3/m256}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      Adds packed double-precision floating-point values from \i{ymm3/m256} and \i{ymm2}.
      Stores the result in \i{ymm1}.
  - opcode: EVEX.128.66.0F.W1 58 /r
    mnemonic: VADDPD \i{xmm1} {k1}{z}, \i{xmm2}, \i{xmm3/m128/m64bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: [AVX512VL, AVX512F]
    description: >-
      Adds packed double-precision floating-point values from \i{xmm3/m128/m64bcst} and \i{xmm2}.
      Stores the result in \i{xmm1} using writemask \i{k1}.
  - opcode: EVEX.256.66.0F.W1 58 /r
    mnemonic: VADDPD \i{ymm1} {k1}{z}, \i{ymm2}, \i{ymm3/m256/m64bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: [AVX512VL, AVX512F]
    description: >-
      Adds packed double-precision floating-point values from \i{ymm3/m256/m64bcst} and \i{ymm2}.
      Stores the result in \i{ymm1} using writemask \i{k1}.
  - opcode: EVEX.512.66.0F.W1 58 /r
    mnemonic: VADDPD \i{zmm1} {k1}{z}, \i{zmm2}, \i{zmm3/m512/m64bcst} {er}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX512F
    description: >-
      Adds packed double-precision floating-point values from \i{zmm3/m512/m64bcst} and \i{zmm2}.
      Stores the result in \i{zmm1} using writemask \i{k1}.
encoding:
  operands: 3
  hasTuple: true
  encodings:
    LEGACY:
      - N/A
      - ModRM.reg[rw]
      - ModRM.r/m[r]
      - ""
    VEX:
      - N/A
      - ModRM.reg[w]
      - VEX.vvvv[r]
      - ModRM.r/m[r]
    EVEX:
      - Full
      - ModRM.reg[w]
      - EVEX.vvvv[r]
      - ModRM.r/m[r]
bitEncoding:
  list:
    - form: xmmreg3(2) into xmmreg3(1)
      bits:
        - \bits{66}
        - \bits{0F}
        - \bits{58}
        - \bits{11 xmmreg3(1) xmmreg3(2)}
    - form: memory into xmmreg3
      bits:
        - \bits{66}
        - \bits{0F}
        - \bits{58}
        - \bits{mod xmmreg3 r/m}
description: >-
  The \c{(V)ADDPD} instruction adds two, four, or eight double-precision floating-point values from the first source operand to the second source operand.
  The result is stored in in the destination operand.

  All versions \i{except} the legacy SSE version zero the unused upper SIMD register bits.
operation: |-
  public void ADDPD(SimdF64 dest, SimdF64 src)
  {
    dest[0] += src[0];
    dest[1] += src[1];
    // dest[2..Simd.Max] (unmodified)
  }

  void VADDPD_Vex(SimdF64 dest, SimdF64 src1, SimdF64 src2, int kl)
  {
    for (int n = 0; n < kl, n++)
      dest[n] = src1[n] + src2[n];
    dest[kl..Simd.MAX] = 0;
  }
  public void VADDPD_Vex128(SimdF64 dest, SimdF64 src1, SimdF64 src2)
  {
    VADDPD_Vex(dest, src1, src2, 2);
  }
  public void VADDPD_Vex256(SimdF64 dest, SimdF64 src1, SimdF64 src2)
  {
    VADDPD_Vex(dest, src1, src2, 4);
  }

  void VADDPD_EvexMemory(SimdF64 dest, SimdF64 src1, SimdF64 src2, KMask k, int kl)
  {
    for (int n = 0; n < kl, n++)
    {
      if (k[n])
      {
        if (EVEX.b)
          dest[n] = src1[n] + src2[0];
        else
          dest[n] = src1[n] + src2[n];
      }
      else if (EVEX.z)
      {
        dest[n] = 0;
      }
    }
    dest[kl..Simd.MAX] = 0;
  }
  public void VADDPD_Evex128Memory(SimdF64 dest, SimdF64 src1, SimdF64 src2, KMask k)
  {
    VADDPD_EvexMemory(dest, src1, src2, k, 2);
  }
  public void VADDPD_Evex256Memory(SimdF64 dest, SimdF64 src1, SimdF64 src2, KMask k)
  {
    VADDPD_EvexMemory(dest, src1, src2, k, 4);
  }
  public void VADDPD_Evex512Memory(SimdF64 dest, SimdF64 src1, SimdF64 src2, KMask k)
  {
    VADDPD_EvexMemory(dest, src1, src2, k, 8);
  }

  void VADDPD_EvexRegister(SimdF64 dest, SimdF64 src1, SimdF64 src2, KMask k, int kl)
  {
    if (kl == 8 && EVEX.b)
      SetRoundingForThisInstruction(EVEX.rc);
    else
      SetRoundingForThisInstruction(MXCSR.rc);

    for (int n = 0; n < kl, n++)
    {
      if (k[n])
        dest[n] = src1[n] + src2[n];
      else if (EVEX.z)
        dest[n] = 0;
    }
    dest[kl..Simd.MAX] = 0;
  }
  public void VADDPD_Evex128Register(SimdF64 dest, SimdF64 src1, SimdF64 src2, KMask k)
  {
    VADDPD_EvexRegister(dest, src1, src2, k, 2);
  }
  public void VADDPD_Evex256Register(SimdF64 dest, SimdF64 src1, SimdF64 src2, KMask k)
  {
    VADDPD_EvexRegister(dest, src1, src2, k, 4);
  }
  public void VADDPD_Evex512Register(SimdF64 dest, SimdF64 src1, SimdF64 src2, KMask k)
  {
    VADDPD_EvexRegister(dest, src1, src2, k, 8);
  }
intrinsics: |-
  __m128d _mm_add_pd(__m128d a, __m128d b)
  __m128d _mm_mask_add_pd(__m128d s, __mmask8 k, __m128d a, __m128d b)
  __m128d _mm_maskz_add_pd(__mmask8 k, __m128d a, __m128d b)

  __m256d _mm256_add_pd(__m256d a, __m256d b)
  __m256d _mm256_mask_add_pd(__m256d s, __mmask8 k, __m256d a, __m256d b)
  __m256d _mm256_maskz_add_pd(__mmask8 k, __m256d a, __m256d b)

  __m512d _mm512_add_pd(__m512d a, __m512d b)
  __m512d _mm512_add_round_pd(__m512d a, __m512d b, int32_t rounding)
  __m512d _mm512_mask_add_pd(__m512d s, __mmask8 k, __m512d a, __m512d b)
  __m512d _mm512_mask_add_round_pd(__m512d s, __mmask8 k, __m512d a, __m512d b, int32_t rounding)
  __m512d _mm512_maskz_add_pd(__mmask8 k, __m512d a, __m512d b)
  __m512d _mm512_maskz_add_round_pd(__mmask8 k, __m512d a, __m512d b, int32_t rounding)
exceptions:
  floating: Overflow, Underflow, Invalid, Precision, Denormal
  other:
    - "VEX encoded form: see Exceptions Type 2."
    - "EVEX encoded form: see Exceptions Type E2."
