%YAML 1.2
---
title: Logical AND Packed Double-Precision Floating-Point Values
opcode:
  - opcode: 66 0F 54 /r
    mnemonic: ANDPD \i{xmm1}, \i{xmm2/m128}
    encoding: LEGACY
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: SSE2
    description: >-
      ANDs packed double-precision floating-point values from \i{xmm2/m128} and \i{xmm1}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.128.66.0F.WIG 54 /r
    mnemonic: VANDPD \i{xmm1}, \i{xmm2}, \i{xmm3/m128}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      ANDs packed double-precision floating-point values from \i{xmm3/m128} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: VEX.256.66.0F.WIG 54 /r
    mnemonic: VANDPD \i{ymm1}, \i{ymm2}, \i{ymm3/m256}
    encoding: VEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX
    description: >-
      ANDs packed double-precision floating-point values from \i{ymm3/m256} and \i{ymm2}.
      Stores the result in \i{ymm1}.
  - opcode: EVEX.128.66.0F.W1 54 /r
    mnemonic: VANDPD \i{xmm1} {k1}{z}, \i{xmm2}, \i{xmm3/m128/m64bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: [AVX512VL, AVS512DQ]
    description: >-
      ANDs packed double-precision floating-point values from \i{xmm3/m128/m64bcst} and \i{xmm2}.
      Stores the result in \i{xmm1}.
  - opcode: EVEX.256.66.0F.W1 54 /r
    mnemonic: VANDPD \i{ymm1} {k1}{z}, \i{ymm2}, \i{ymm3/m256/m64bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: [AVX512VL, AVS512DQ]
    description: >-
      ANDs packed double-precision floating-point values from \i{ymm3/m256/m64bcst} and \i{ymm2}.
      Stores the result in \i{ymm1}.
  - opcode: EVEX.512.66.0F.W1 54 /r
    mnemonic: VANDPD \i{zmm1} {k1}{z}, \i{zmm2}, \i{zmm3/m512/m64bcst}
    encoding: EVEX
    validity:
      16: invalid
      32: valid
      64: valid
    cpuid: AVX512DQ
    description: >-
      ANDs packed double-precision floating-point values from \i{zmm3/m512/m64bcst} and \i{zmm2}.
      Stores the result in \i{zmm1}.
encoding:
  operands: 3
  hasTuple: true
  encodings:
    LEGACY:
      - N/A
      - ModRM.reg[rw]
      - ModRM.r/m[r]
      - ""
    VEX:
      - N/A
      - ModRM.reg[rw]
      - VEX.vvvv[r]
      - ModRM.r/m[r]
    EVEX:
      - Full
      - ModRM.reg[rw]
      - EVEX.vvvv[r]
      - ModRM.r/m[r]
bitEncoding:
  list:
    - form: xmmreg3(2) into xmmreg3(1)
      bits:
        - \bits{66}
        - \bits{0F}
        - \bits{54}
        - \bits{11 xmmreg3(1) xmmreg3(2)}
    - form: memory into xmmreg3
      bits:
        - \bits{66}
        - \bits{0F}
        - \bits{54}
        - \bits{mod xmmreg3 r/m}
description: >-
  The \c{(V)ANDPD} instruction ANDs two, four, or eight double-precision floating-point values from the first source operand to the second source operand.
  The result is stored in in the destination operand.

  This instruction, despite being named as if it operates on floating-point numbers, internally operates on 64 bit integers.
  The "Operation" section below has been updated to reflect this (using \c{SimdU64} instead of \c{SimdF64}).

  All versions \i{except} the legacy SSE version zero the unused upper SIMD register bits.
operation: |-
  public void ANDPD(SimdU64 dest, SimdU64 src)
  {
    dest[0] &= src[0];
    dest[1] &= src[1];
    // dest[2..Simd.Max] (unmodified)
  }

  void VANDPD_Vex(SimdU64 dest, SimdU64 src1, SimdU64 src2, int kl)
  {
    for (int n = 0; n < kl, n++)
      dest[n] = src1[n] & src2[n];
    dest[kl..Simd.MAX] = 0;
  }
  public void VANDPD_Vex128(SimdU64 dest, SimdU64 src1, SimdU64 src2)
  {
    VANDPD_Vex(dest, src1, src2, 2);
  }
  public void VANDPD_Vex256(SimdU64 dest, SimdU64 src1, SimdU64 src2)
  {
    VANDPD_Vex(dest, src1, src2, 4);
  }

  void VANDPD_EvexMemory(SimdU64 dest, SimdU64 src1, SimdU64 src2, KMask k, int kl)
  {
    for (int n = 0; n < kl, n++)
    {
      if (k[n])
      {
        if (EVEX.b)
          dest[n] = src1[n] & src2[0];
        else
          dest[n] = src1[n] & src2[n];
      }
      else if (EVEX.z)
      {
        dest[n] = 0;
      }
    }
    dest[kl..Simd.MAX] = 0;
  }
  public void VANDPD_Evex128Memory(SimdU64 dest, SimdU64 src1, SimdU64 src2, KMask k)
  {
    VANDPD_EvexMemory(dest, src1, src2, k, 2);
  }
  public void VANDPD_Evex256Memory(SimdU64 dest, SimdU64 src1, SimdU64 src2, KMask k)
  {
    VANDPD_EvexMemory(dest, src1, src2, k, 4);
  }
  public void VANDPD_Evex512Memory(SimdU64 dest, SimdU64 src1, SimdU64 src2, KMask k)
  {
    VANDPD_EvexMemory(dest, src1, src2, k, 8);
  }

  void VANDPD_EvexRegister(SimdU64 dest, SimdU64 src1, SimdU64 src2, KMask k, int kl)
  {
    for (int n = 0; n < kl, n++)
    {
      if (k[n])
        dest[n] = src1[n] & src2[n];
      else if (EVEX.z)
        dest[n] = 0;
    }
    dest[kl..Simd.MAX] = 0;
  }
  public void VANDPD_Evex128Register(SimdU64 dest, SimdU64 src1, SimdU64 src2, KMask k)
  {
    VANDPD_EvexRegister(dest, src1, src2, k, 2);
  }
  public void VANDPD_Evex256Register(SimdU64 dest, SimdU64 src1, SimdU64 src2, KMask k)
  {
    VANDPD_EvexRegister(dest, src1, src2, k, 4);
  }
  public void VANDPD_Evex512Register(SimdU64 dest, SimdU64 src1, SimdU64 src2, KMask k)
  {
    VANDPD_EvexRegister(dest, src1, src2, k, 8);
  }
intrinsicsC: |-
  __m128d _mm_and_pd(__m128d a, __m128d b)
  __m128d _mm_mask_and_pd(__m128d s, __mmask8 k, __m128d a, __m128d b)
  __m128d _mm_maskz_and_pd(__mmask8 k, __m128d a, __m128d b)

  __m256d _mm256_and_pd(__m256d a, __m256d b)
  __m256d _mm256_mask_and_pd(__m256d s, __mmask8 k, __m256d a, __m256d b)
  __m256d _mm256_maskz_and_pd(__mmask8 k, __m256d a, __m256d b)

  __m512d _mm512_and_pd(__m512d a, __m512d b)
  __m512d _mm512_mask_and_pd(__m512d s, __mmask8 k, __m512d a, __m512d b)
  __m512d _mm512_maskz_and_pd(__mmask8 k, __m512d a, __m512d b)
exceptions:
  floating: None
  other:
    - "VEX encoded form: see Exceptions Type 4."
    - "EVEX encoded form: see Exceptions Type E4."
